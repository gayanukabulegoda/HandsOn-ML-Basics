{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01182017",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Clustering Wikipedia Articles with K-Means\n",
    "\n",
    "This notebook demonstrates how to fetch text documents from Wikipedia, process it, and then use **K-Means** to discover topic clusters. The key steps are:\n",
    "\n",
    "**Setup:** Installing and importing the required libraries.\n",
    "\n",
    "**Data Collection:** Fetching articles from Wikipedia.\n",
    "\n",
    "**Text Preprocessing:** Cleaning the text data.\n",
    "\n",
    "**Vectorization:** Converting text into numerical TF-IDF vectors.\n",
    "\n",
    "**Clustering:** Applying the K-Means algorithm.\n",
    "\n",
    "**Analysis:** Inspecting the clusters to understand their topics.\n",
    "\n",
    "**Prediction:** Categorizing a new document using the trained model.\n",
    "\n",
    "You may need to install the libraries below.\n",
    "\n",
    "pip3 install wikipedia-api\n",
    "\n",
    "pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be6bfc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Imports ---\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94265611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not necessary (If there is any error, use this code set)\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7827cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded successfully using the manual method.\n",
      "Libraries imported and NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GRB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GRB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\GRB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Now, try to download the data again\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"NLTK data downloaded successfully using the manual method.\")\n",
    "\n",
    "print(\"Libraries imported and NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683f88fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched: Galaxy\n",
      "Successfully fetched: Black hole\n",
      "Successfully fetched: Supernova\n",
      "Successfully fetched: DNA\n",
      "Successfully fetched: Photosynthesis\n",
      "Successfully fetched: Evolution\n",
      "Successfully fetched: Machine learning\n",
      "Successfully fetched: Artificial intelligence\n",
      "Successfully fetched: Computer programming\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Fetch Wikipedia Articles ---\n",
    "\n",
    "# List of articles to cluster. We've chosen topics in astronomy, biology, and computer science.\n",
    "article_titles = [\n",
    "    \"Galaxy\", \"Black hole\", \"Supernova\", # Astronomy\n",
    "    \"DNA\", \"Photosynthesis\", \"Evolution\", # Biology\n",
    "    \"Machine learning\", \"Artificial intelligence\", \"Computer programming\" # Computer Science\n",
    "]\n",
    "\n",
    "# Initialize the Wikipedia API\n",
    "wiki_api = wikipediaapi.Wikipedia('MyClusteringProject/1.0', 'en')\n",
    "\n",
    "documents = []\n",
    "for title in article_titles:\n",
    "    page = wiki_api.page(title)\n",
    "    if page.exists():\n",
    "        documents.append(page.text)\n",
    "        print(f\"Successfully fetched: {title}\")\n",
    "    else:\n",
    "        print(f\"Could not find page: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b89a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Preprocess the Text ---\n",
    "\n",
    "# Data preprocessing means cleaning and preparing text data for analysis. This involves removing noise, normalizing text, and extracting relevant features.\n",
    "# Note: Data gaining (Data lookup) from set is faster than list\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Stop words (Examples: \"the\", \"is\", \"in\")\n",
    "lemmatizer = WordNetLemmatizer() # Lemmatizer - Converts words to their base form (Examples: \"running\", \"ran\", \"runs\" -> \"run\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize the sentence to words (split by whitespace) and assign to a list\n",
    "    words = text.split()\n",
    "    # Remove stop words and lemmatize inorder to convert words to their base form and join to return as a single string\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "processed_documents = [preprocess_text(doc) for doc in documents] # Preprocess each document and append to the list\n",
    "print(\"Text preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191bf565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 5187 stored elements and shape (9, 1000)>\n",
      "  Coords\tValues\n",
      "  (0, 380)\t0.8694624356020488\n",
      "  (0, 906)\t0.01159671739286774\n",
      "  (0, 871)\t0.28333919626648235\n",
      "  (0, 876)\t0.025000517317630797\n",
      "  (0, 777)\t0.004908737339371123\n",
      "  (0, 471)\t0.026998055366541176\n",
      "  (0, 383)\t0.06293190367536866\n",
      "  (0, 270)\t0.029452424036226738\n",
      "  (0, 214)\t0.039269898714968984\n",
      "  (0, 552)\t0.035461125238748774\n",
      "  (0, 107)\t0.008760308894358658\n",
      "  (0, 932)\t0.010638337571624632\n",
      "  (0, 402)\t0.016667011545087197\n",
      "  (0, 992)\t0.005319168785812316\n",
      "  (0, 571)\t0.13611392761821212\n",
      "  (0, 763)\t0.002899179348216935\n",
      "  (0, 982)\t0.0710298940313149\n",
      "  (0, 192)\t0.005319168785812316\n",
      "  (0, 853)\t0.013140463341537987\n",
      "  (0, 309)\t0.009833109949276352\n",
      "  (0, 572)\t0.015945486415193142\n",
      "  (0, 742)\t0.004806957568994388\n",
      "  (0, 843)\t0.019503618881311826\n",
      "  (0, 271)\t0.05555670515029067\n",
      "  (0, 512)\t0.007247948370542337\n",
      "  :\t:\n",
      "  (8, 4)\t0.05115613811068583\n",
      "  (8, 918)\t0.03654009865048988\n",
      "  (8, 635)\t0.02481347865902978\n",
      "  (8, 91)\t0.007308019730097976\n",
      "  (8, 667)\t0.016542319106019852\n",
      "  (8, 493)\t0.5293542113926353\n",
      "  (8, 722)\t0.4797272540745757\n",
      "  (8, 183)\t0.016542319106019852\n",
      "  (8, 530)\t0.02481347865902978\n",
      "  (8, 508)\t0.008271159553009926\n",
      "  (8, 937)\t0.008271159553009926\n",
      "  (8, 444)\t0.008271159553009926\n",
      "  (8, 181)\t0.008271159553009926\n",
      "  (8, 441)\t0.041355797765049634\n",
      "  (8, 569)\t0.02481347865902978\n",
      "  (8, 961)\t0.09098275508310918\n",
      "  (8, 399)\t0.008271159553009926\n",
      "  (8, 482)\t0.02481347865902978\n",
      "  (8, 186)\t0.008271159553009926\n",
      "  (8, 412)\t0.04962695731805956\n",
      "  (8, 164)\t0.04962695731805956\n",
      "  (8, 487)\t0.009512862746690822\n",
      "  (8, 73)\t0.009512862746690822\n",
      "  (8, 242)\t0.05707717648014494\n",
      "  (8, 721)\t0.29489874514741554\n",
      "TF-IDF matrix created successfully.\n",
      "Shape of the matrix: (9, 1000)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Convert Text to Vectors ---\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer - To convert the preprocessed text documents into numerical feature vectors\n",
    "# Here TF stands for Term Frequency - measures how frequently a term occurs in a document (Frequent words contribute more to the document's representation)\n",
    "# IDF stands for Inverse Document Frequency - measures how important a word is to a document in a collection (Rare words contribute more to the document's representation)\n",
    "vectorizer = TfidfVectorizer(max_features=1000) # Limit to the top 1000 features\n",
    "\n",
    "# So here it creates 09 rows (for 09 documents) with 1000 columns (for 1000 features) vectorized representation (Sparse Matrix - Table)\n",
    "\n",
    "# Create the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "\n",
    "print(tfidf_matrix) # In the Matrix it prints only non-zero values\n",
    "\n",
    "print(\"TF-IDF matrix created successfully.\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d64de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Run K-Means ---\n",
    "\n",
    "k = 3  # Number of clusters (Astronomy, Biology, Computer Science)\n",
    "\n",
    "# Since we mention KMeans, in scikit learn by default it runs KMeans++ (which is an improved version of KMeans)\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "## The below line is functionally identical to the above line\n",
    "# kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n",
    "# Here n_init is the number of times the algorithm will be run with different centroid seeds - in-order to choose the best one\n",
    "\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get the cluster assignments for each document\n",
    "labels = kmeans.labels_\n",
    "print(labels) # Labels indicate which cluster each document belongs to (0, 1, or 2) - Záµ¢ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aacedcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cluster 0 ---\n",
      "Documents: ['DNA', 'Photosynthesis', 'Evolution']\n",
      "Top Keywords: ['dna', 'organism', 'photosynthesis', 'specie', 'gene', 'plant', 'photosynthetic', 'evolution', 'carbon', 'cell']\n",
      "\n",
      "--- Cluster 1 ---\n",
      "Documents: ['Machine learning', 'Artificial intelligence', 'Computer programming']\n",
      "Top Keywords: ['learning', 'ai', 'language', 'programming', 'machine', 'data', 'algorithm', 'intelligence', 'program', 'computer']\n",
      "\n",
      "--- Cluster 2 ---\n",
      "Documents: ['Galaxy', 'Black hole', 'Supernova']\n",
      "Top Keywords: ['galaxy', 'star', 'supernova', 'hole', 'black', 'mass', 'type', 'collapse', 'milky', 'light']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Analyze the Results ---\n",
    "\n",
    "# Group document titles by cluster\n",
    "clusters = {i: [] for i in range(k)}\n",
    "for i, label in enumerate(labels):\n",
    "    clusters[label].append(article_titles[i])\n",
    "\n",
    "# Get the top terms per cluster\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"--- Cluster {i} ---\")\n",
    "    print(f\"Documents: {clusters[i]}\")\n",
    "    \n",
    "    top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    print(f\"Top Keywords: {top_terms}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a5b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: algorithm set welldefined instruction designed perform specific task solve computational problem computer science study algorithm fundamental creating efficient scalable software data structure array hash table used organize data way allows algorithm access manipulate effectively\n",
      "\n",
      "Shape of the new vector: (1, 1000)\n",
      "\n",
      "The new document belongs to cluster: 1\n"
     ]
    }
   ],
   "source": [
    "# --- Step 7: Putting the Model to Work - Predicting on New Documents ---\n",
    "# Now for the exciting part! We can take our final \"trained\" model and \n",
    "# use it to instantly categorize a brand new, unseen document. \n",
    "# Let's see which topic cluster it belongs to!\n",
    "\n",
    "# --- Define your new document ---\n",
    "new_text = \"An algorithm is a set of well-defined instructions designed to perform a specific task or solve a computational problem. In computer science, the study of algorithms is fundamental to creating efficient and scalable software. Data structures, such as arrays and hash tables, are used to organize data in a way that allows these algorithms to access and manipulate it effectively.\"\n",
    "\n",
    "# --- Apply the SAME preprocessing ---\n",
    "# We use the preprocess_text function we defined earlier\n",
    "processed_new_text = preprocess_text(new_text)\n",
    "print(f\"Cleaned Text: {processed_new_text}\")\n",
    "\n",
    "# --- Use the FITTED vectorizer to transform the text ---\n",
    "# IMPORTANT: Use .transform(), not .fit_transform()\n",
    "# This ensures it uses the same vocabulary learned from the original documents.\n",
    "new_tfidf_vector = vectorizer.transform([processed_new_text])\n",
    "\n",
    "print(f\"\\nShape of the new vector: {new_tfidf_vector.shape}\")\n",
    "\n",
    "# --- Now you can predict its cluster ---\n",
    "predicted_label = kmeans.predict(new_tfidf_vector)\n",
    "\n",
    "print(f\"\\nThe new document belongs to cluster: {predicted_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
